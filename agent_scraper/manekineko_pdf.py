"""
ã¾ã­ãã­ã“PDFæ–™é‡‘è¡¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (Chain of Thoughtå¼·åŒ–ç‰ˆ + ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½)
=====================================================
"""

import requests
import pdfplumber
import re
import json
import io
import sys
import time
import os
from pathlib import Path
from dotenv import load_dotenv
import google.generativeai as genai
from bs4 import BeautifulSoup

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåº—èˆ—ãƒªã‚¹ãƒˆ
TARGET_STORES = [
    {
        "name": "ã‚«ãƒ©ã‚ªã‚±ã¾ã­ãã­ã“ é˜ªæ€¥æ±é€šã‚Šåº—",
        "url": "https://www.karaokemanekineko.jp/locations/osaka/osaka-shi/hankyu-higashidori-store/"
    },
    {
        "name": "ã‚«ãƒ©ã‚ªã‚±ã¾ã­ãã­ã“ æ¢…ç”°èŠç”°åº—",
        "url": "https://www.karaokemanekineko.jp/locations/osaka/osaka-shi/umeda-shibata-store/"
    },
    {
        "name": "ã‚«ãƒ©ã‚ªã‚±ã¾ã­ãã­ã“ èŒ¶å±‹ç”ºåº—",
        "url": "https://www.karaokemanekineko.jp/locations/osaka/osaka-shi/chayamachi-store/"
    },
    {
        "name": "ã‚«ãƒ©ã‚ªã‚±ã¾ã­ãã­ã“ é˜ªæ€¥æ±é€šã‚Š2å·åº—",
        "url": "https://www.karaokemanekineko.jp/locations/osaka/osaka-shi/hankyuhigashidori-2nd-store/"
    }
]

def fetch_pdf_url(store_url):
    """BeautifulSoupã‚’ä½¿ç”¨ã—ã¦PDFãƒªãƒ³ã‚¯ã‚’ç¢ºå®Ÿã«å–å¾—"""
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(store_url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # .pdf ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’å…¨ã¦æ¢ã™
        links = soup.find_all('a', href=re.compile(r'\.pdf$', re.I))
        
        for link in links:
            url = link.get('href')
            if not url: continue
            
            # ç›¸å¯¾ãƒ‘ã‚¹ãªã‚‰çµ¶å¯¾ãƒ‘ã‚¹ã«
            if not url.startswith('http'):
                url = "https://www.karaokemanekineko.jp" + url
            
            # CloudFrontãªã©ã®URLã‚‚è¨±å¯
            return url
            
        return None
    except Exception as e:
        print(f"Error fetching HTML for {store_url}: {e}", file=sys.stderr)
        return None

def download_pdf(pdf_url):
    try:
        response = requests.get(pdf_url, timeout=15)
        response.raise_for_status()
        return response.content
    except Exception as e:
        print(f"Error downloading PDF {pdf_url}: {e}", file=sys.stderr)
        return None

def extract_prices_with_gemini(pdf_bytes):
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        print("Error: GOOGLE_API_KEY not found.", file=sys.stderr)
        return None

    try:
        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
            if not pdf.pages: return None
            
            # 1ãƒšãƒ¼ã‚¸ç›®ã®ã¿è§£æï¼ˆé€šå¸¸ã“ã“ã«ãƒ¡ã‚¤ãƒ³æ–™é‡‘ãŒã‚ã‚‹ï¼‰
            page = pdf.pages[0]
            im = page.to_image(resolution=300)
            target_image = im.original
            
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.0-flash') # é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
            
            # æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’å«ã‚ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = """
            ã‚ãªãŸã¯ã‚«ãƒ©ã‚ªã‚±æ–™é‡‘ã®å°‚é–€å®¶ã§ã™ã€‚ã“ã®ç”»åƒã®æ–™é‡‘è¡¨ã‹ã‚‰ã€ä»¥ä¸‹ã®æ¡ä»¶ã«åˆã†ã€Œæ•°å€¤ã€ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
            è¡¨ã¯è¤‡é›‘ã§ã€å­¦ç”Ÿãƒ»ã‚·ãƒ‹ã‚¢ãƒ»ä¼šå“¡ãƒ»éä¼šå“¡ãƒ»æœã†ãŸãƒ»ã‚¼ãƒ­ã‚«ãƒ©ãªã©ã®æƒ…å ±ãŒæ··åœ¨ã—ã¦ã„ã¾ã™ã€‚

            ## æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—
            1. è¡¨ã®ã€Œåˆ—ï¼ˆæ¨ªè»¸ï¼‰ã€ã‚’ç¢ºèªã—ã€ã€Œä¸€èˆ¬ä¼šå“¡ï¼ˆMemberï¼‰ã€ã®åˆ—ã‚’è¦‹ã¤ã‘ã‚‹ã€‚â€»å­¦ç”Ÿã‚„ã‚·ãƒ‹ã‚¢ã§ã¯ãªã„ã€‚
            2. è¡¨ã®ã€Œè¡Œï¼ˆç¸¦è»¸ï¼‰ã€ã‚’ç¢ºèªã—ã€ã€Œæ˜¼ï¼ˆOPENã€œ18:00é ƒï¼‰ã€ã®è¡Œã‚’è¦‹ã¤ã‘ã‚‹ã€‚
            3. ãã®äº¤å·®ã™ã‚‹ã‚»ãƒ«ã®ã€Œ30åˆ†æ–™é‡‘ã€ã¨ã€Œãƒ•ãƒªãƒ¼ã‚¿ã‚¤ãƒ æ–™é‡‘ã€ã‚’èª­ã‚€ã€‚
            4. ã€Œãƒ¯ãƒ³ãƒ‰ãƒªãƒ³ã‚¯åˆ¶(+Order)ã€ã‹ã€Œãƒ‰ãƒªãƒ³ã‚¯ãƒãƒ¼ä»˜ã€ã‹ã¯å•ã‚ãšã€è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹é‡‘é¡ï¼ˆå®¤æ–™ï¼‰ã‚’ãã®ã¾ã¾æŠ½å‡ºã™ã‚‹ã€‚
            5. åœŸæ—¥ç¥(Weekend)ã§ã¯ãªãã€**å¹³æ—¥(Weekday)** ã®æ–™é‡‘ã‚’å„ªå…ˆã™ã‚‹ã€‚

            ## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (JSONã®ã¿)
            ```json
            {
                "reasoning": "è¡¨ã®å·¦å´ã«ã‚ã‚‹æ™‚é–“å¸¯... ä¼šå“¡åˆ—ã®...",
                "weekday_30min": æ•°å€¤,
                "weekday_free_time": æ•°å€¤ ã¾ãŸã¯ null
            }
            ```
            â€»æ•°å€¤ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ nullã€‚æ–‡å­—ï¼ˆ"å††"ãªã©ï¼‰ã¯å‰Šé™¤ã—ã¦æ•°å€¤ã®ã¿ã«ã™ã‚‹ã€‚
            """

            response = model.generate_content([prompt, target_image])
            text = response.text.replace("```json", "").replace("```", "").strip()
            
            try:
                data = json.loads(text)
                print(f"  ğŸ¤– AI Thinking: {data.get('reasoning')}", file=sys.stderr)
                return data
            except json.JSONDecodeError:
                print(f"  Failed to parse JSON: {text}", file=sys.stderr)
                return None

    except Exception as e:
        print(f"Error in Gemini: {e}", file=sys.stderr)
        return None

def main():
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    json_path = "data/stations_with_prices.json"
    cache_map = {}
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for station, stores in data.get("stations", {}).items():
                    for store in stores:
                        # æ¤œç´¢ç”¨ã«æ­£è¦åŒ–
                        norm_name = store.get("name", "").replace(" ", "").replace("ã€€", "")
                        cache_map[norm_name] = store
        except Exception as e:
            print(f"Warning: Failed to load cache: {e}", file=sys.stderr)

    results = []
    print("[", file=sys.stdout)
    first = True

    for store in TARGET_STORES:
        print(f"Processing {store['name']}...", file=sys.stderr)
        pdf_url = fetch_pdf_url(store['url'])
        
        pricing_data = {"status": "failed"}
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        norm_target = store['name'].replace(" ", "").replace("ã€€", "")
        cached_store = cache_map.get(norm_target)
        
        # éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒƒãƒ—ã«ãªã„å ´åˆ)
        if not cached_store:
             for k, v in cache_map.items():
                if norm_target in k or k in norm_target:
                    if "ã¾ã­ãã­ã“" in v.get("name", ""):
                        cached_store = v
                        break

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆåˆ¤å®š
        # URLãŒä¸€è‡´ã—ã€ã‹ã¤ä»¥å‰ã®å–å¾—ãŒã€ŒæˆåŠŸã€ã—ã¦ã„ã‚‹å ´åˆ
        if pdf_url and cached_store:
            old_url = cached_store.get("pdf_url")
            old_status = cached_store.get("pricing", {}).get("status")
            
            if old_url == pdf_url and old_status == "success":
                print(f"  âœ¨ Cache Hit! PDF has not changed. Using existing data.", file=sys.stderr)
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä½¿ã†
                pricing_data = cached_store["pricing"]
                
                # çµæœå‡ºåŠ›ã—ã¦æ¬¡ã¸
                result = {
                    "store_name": store['name'],
                    "pdf_url": pdf_url, 
                    "pricing": pricing_data
                }
                if not first: print(",", file=sys.stdout)
                print(json.dumps(result, ensure_ascii=False, indent=2), file=sys.stdout)
                sys.stdout.flush()
                first = False
                continue

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ -> å®Ÿå‡¦ç†
        if pdf_url:
            print(f"  PDF Found: {pdf_url}", file=sys.stderr)
            pdf_bytes = download_pdf(pdf_url)
            
            if pdf_bytes:
                extracted = extract_prices_with_gemini(pdf_bytes)
                if extracted and extracted.get("weekday_30min"):
                    pricing_data = {
                        "status": "success",
                        "day": {
                            "30min": {"member": extracted["weekday_30min"], "general": None},
                            "free_time": {"member": extracted["weekday_free_time"], "general": None}
                        }
                    }
                    print(f"  âœ… Extracted: 30min={extracted['weekday_30min']}, Free={extracted['weekday_free_time']}", file=sys.stderr)
                else:
                    print("  âŒ Extraction failed or returned null.", file=sys.stderr)
        else:
            print("  âŒ PDF not found.", file=sys.stderr)

        result = {
            "store_name": store['name'],
            "pdf_url": pdf_url,
            "pricing": pricing_data
        }
        
        if not first: print(",", file=sys.stdout)
        print(json.dumps(result, ensure_ascii=False, indent=2), file=sys.stdout)
        sys.stdout.flush()
        first = False
        time.sleep(5) # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

    print("]", file=sys.stdout)

if __name__ == "__main__":
    main()
