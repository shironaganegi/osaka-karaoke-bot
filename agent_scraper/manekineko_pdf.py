import argparse
import requests
import pdfplumber
import re
import json
import io
import sys
import time
import os
from pathlib import Path
from dotenv import load_dotenv
import google.generativeai as genai
from bs4 import BeautifulSoup

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
# .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
load_dotenv()

# Windowsç’°å¢ƒã§ã®æ–‡å­—åŒ–ã‘å¯¾ç­– (UTF-8å¼·åˆ¶)
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

def fetch_all_manekineko_stores():
    """stations_with_prices.jsonã‹ã‚‰ã¾ã­ãã­ã“å…¨åº—èˆ—ã‚’å–å¾—"""
    json_path = "data/stations_with_prices.json"
    stores_list = []
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for station, stores in data.get("stations", {}).items():
                    for s in stores:
                        if s.get("chain") == "manekineko":
                            stores_list.append({
                                "name": s.get("name"),
                                "url": s.get("url")
                            })
        except Exception as e:
            print(f"Error loading stores: {e}", file=sys.stderr)
    return stores_list

def fetch_pdf_url(store_url):
    """BeautifulSoupã‚’ä½¿ç”¨ã—ã¦PDFãƒªãƒ³ã‚¯ã‚’ç¢ºå®Ÿã«å–å¾—"""
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(store_url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # .pdf ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’å…¨ã¦æ¢ã™
        links = soup.find_all('a', href=re.compile(r'\.pdf$', re.I))
        
        for link in links:
            url = link.get('href')
            if not url: continue
            
            # ç›¸å¯¾ãƒ‘ã‚¹ãªã‚‰çµ¶å¯¾ãƒ‘ã‚¹ã«
            if not url.startswith('http'):
                url = "https://www.karaokemanekineko.jp" + url
            
            # CloudFrontãªã©ã®URLã‚‚è¨±å¯
            return url
            
        return None
    except Exception as e:
        print(f"Error fetching HTML for {store_url}: {e}", file=sys.stderr)
        return None

def download_pdf(pdf_url):
    try:
        response = requests.get(pdf_url, timeout=15)
        response.raise_for_status()
        return response.content
    except Exception as e:
        print(f"Error downloading PDF {pdf_url}: {e}", file=sys.stderr)
        return None

def extract_prices_with_gemini(pdf_bytes):
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        print("Error: GOOGLE_API_KEY not found.", file=sys.stderr)
        return None

    try:
        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
            if not pdf.pages: return None
            
            # 1ãƒšãƒ¼ã‚¸ç›®ã®ã¿è§£æï¼ˆé€šå¸¸ã“ã“ã«ãƒ¡ã‚¤ãƒ³æ–™é‡‘ãŒã‚ã‚‹ï¼‰
            page = pdf.pages[0]
            im = page.to_image(resolution=150)
            target_image = im.original
            
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.0-flash-lite') # è»½é‡ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
            
            # æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’å«ã‚ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (å¼·åŒ–ç‰ˆ)
            prompt = """
            ã‚ãªãŸã¯ã‚«ãƒ©ã‚ªã‚±æ–™é‡‘ã®å°‚é–€å®¶ã§ã™ã€‚ã“ã®ç”»åƒã®æ–™é‡‘è¡¨ã‹ã‚‰ã€ä»¥ä¸‹ã®æ¡ä»¶ã«åˆã†ã€Œæ•°å€¤ã€ã‚’æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
            
            ## å¿…é ˆæ¡ä»¶
            1. **ã€Œä¸€èˆ¬ï¼ˆéä¼šå“¡ï¼‰ã€ã¨ã€Œä¼šå“¡ã€ã®æ–™é‡‘ã‚’å¿…ãšä¸¡æ–¹æ¢ã—ã¦ãã ã•ã„ã€‚**
            2. è¡¨ã®ä¸­ã‹ã‚‰ã€Œä¸€èˆ¬ã€ã¾ãŸã¯ã€Œéä¼šå“¡ã€ã®åˆ—ã¨ã€ã€Œä¼šå“¡ã€ã¾ãŸã¯ã€Œã‚¢ãƒ—ãƒªä¼šå“¡ã€ã®åˆ—ã‚’æ˜ç¢ºã«åŒºåˆ¥ã—ã¦ãã ã•ã„ã€‚
            3. ã‚‚ã—ã€Œä¸€èˆ¬ã€ã®è¨˜è¼‰ãŒå…¨ããªã„å ´åˆã¯ã€ä¼šå“¡ä¾¡æ ¼ã‹ã‚‰å‹æ‰‹ã«è¨ˆç®—ã›ãšã€å¿…ãš null (å–å¾—ä¸å¯) ã¨ã—ã¦ãã ã•ã„ã€‚é©å½“ãªæ¨æ¸¬ã¯ç¦æ­¢ã§ã™ã€‚
            4. è¡Œï¼ˆç¸¦è»¸ï¼‰ã¯ã€Œæ˜¼ï¼ˆOPENã€œ18:00é ƒï¼‰ã€ã®æ™‚é–“å¸¯ã‚’è¦‹ã¦ãã ã•ã„ã€‚
            5. åœŸæ—¥ç¥(Weekend)ã§ã¯ãªãã€**å¹³æ—¥(Weekday)** ã®æ–™é‡‘ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚
            6. ã€Œ30åˆ†æ–™é‡‘ã€ã¨ã€Œãƒ•ãƒªãƒ¼ã‚¿ã‚¤ãƒ æ–™é‡‘ã€ã®ä¸¡æ–¹ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

            ## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (JSONã®ã¿)
            ```json
            {
                "reasoning": "è¡¨ã®å³å´ã«ã‚ã‚‹ä¸€èˆ¬åˆ—ã‚’ç¢ºèª... ä¼šå“¡åˆ—ã¨æ¯”è¼ƒã—ã¦...",
                "weekday_30min_general": æ•°å€¤ ã¾ãŸã¯ null,
                "weekday_30min_member": æ•°å€¤ ã¾ãŸã¯ null,
                "weekday_free_time_general": æ•°å€¤ ã¾ãŸã¯ null,
                "weekday_free_time_member": æ•°å€¤ ã¾ãŸã¯ null
            }
            ```
            â€»æ•°å€¤ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ nullã€‚æ–‡å­—ï¼ˆ"å††"ãªã©ï¼‰ã¯å‰Šé™¤ã—ã¦æ•°å€¤ã®ã¿ã«ã™ã‚‹ã€‚
            """

            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = model.generate_content([prompt, target_image])
                    text = response.text.replace("```json", "").replace("```", "").strip()
                    
                    try:
                        data = json.loads(text)
                        print(f"  ğŸ¤– AI Thinking: {data.get('reasoning')}", file=sys.stderr)
                        return data
                    except json.JSONDecodeError:
                        print(f"  Failed to parse JSON: {text}", file=sys.stderr)
                        return None
                except Exception as e:
                    if "429" in str(e) or "Resource exhausted" in str(e):
                        wait_time = (attempt + 1) * 20
                        print(f"  âš ï¸ Quota exceeded. Retrying in {wait_time}s...", file=sys.stderr)
                        time.sleep(wait_time)
                    else:
                        raise e
            
            print("  âŒ Max retries exceeded.", file=sys.stderr)
            return None

    except Exception as e:
        print(f"Error in Gemini: {e}", file=sys.stderr)
        return None

def main():
    parser = argparse.ArgumentParser(description='Manekineko PDF Scraper')
    parser.add_argument('--force', action='store_true', help='Force re-download and re-analysis of all PDFs')
    parser.add_argument('--pdf-only', action='store_true', help='Only fetch PDF URLs, skip Gemini extraction')
    parser.add_argument('--output', type=str, help='Output JSON file path (optional)')
    args = parser.parse_args()

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    json_path = "data/stations_with_prices.json"
    cache_map = {}
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for station, stores in data.get("stations", {}).items():
                    for store in stores:
                        norm_name = store.get("name", "").replace(" ", "").replace("ã€€", "")
                        cache_map[norm_name] = store
        except Exception as e:
            print(f"Warning: Failed to load cache: {e}", file=sys.stderr)

    # å…¨åº—èˆ—ãƒªã‚¹ãƒˆã®å–å¾—
    target_stores = fetch_all_manekineko_stores()
    if not target_stores:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæ—¢å­˜ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼‰
        target_stores = [
            {"name": "ã‚«ãƒ©ã‚ªã‚±ã¾ã­ãã­ã“ é˜ªæ€¥æ±é€šã‚Šåº—", "url": "https://www.karaokemanekineko.jp/locations/osaka/osaka-shi/hankyu-higashidori-store/"},
            {"name": "ã‚«ãƒ©ã‚ªã‚±ã¾ã­ãã­ã“ æ¢…ç”°èŠç”°åº—", "url": "https://www.karaokemanekineko.jp/locations/osaka/osaka-shi/umeda-shibata-store/"},
            {"name": "ã‚«ãƒ©ã‚ªã‚±ã¾ã­ãã­ã“ èŒ¶å±‹ç”ºåº—", "url": "https://www.karaokemanekineko.jp/locations/osaka/osaka-shi/chayamachi-store/"},
            {"name": "ã‚«ãƒ©ã‚ªã‚±ã¾ã­ãã­ã“ é˜ªæ€¥æ±é€šã‚Š2å·åº—", "url": "https://www.karaokemanekineko.jp/locations/osaka/osaka-shi/hankyuhigashidori-2nd-store/"}
        ]

    results = []
    # å‡ºåŠ›å…ˆã®è¨­å®š
    out_stream = sys.stdout
    if args.output:
        out_stream = open(args.output, 'w', encoding='utf-8')

    try:
        print("[", file=out_stream)
        first = True
        
        success_count = 0
        general_price_count = 0

        for store in target_stores:
            print(f"Processing {store['name']}...", file=sys.stderr)
            pdf_url = fetch_pdf_url(store['url'])
            
            pricing_data = {"status": "failed"}
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ (Forceãƒ¢ãƒ¼ãƒ‰ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—)
            norm_target = store['name'].replace(" ", "").replace("ã€€", "")
            cached_store = cache_map.get(norm_target)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆåˆ¤å®š
            if not args.force and pdf_url and cached_store:
                old_url = cached_store.get("pdf_url")
                old_status = cached_store.get("pricing", {}).get("status")
                
                if old_url == pdf_url and old_status == "success":
                    print(f"  âœ¨ Cache Hit! PDF has not changed.", file=sys.stderr)
                    pricing_data = cached_store["pricing"]
                    
                    # çµæœå‡ºåŠ›
                    result = {
                        "store_name": store['name'],
                        "pdf_url": pdf_url, 
                        "pricing": pricing_data
                    }
                    if not first: print(",", file=out_stream)
                    print(json.dumps(result, ensure_ascii=False, indent=2), file=out_stream)
                    if args.output: out_stream.flush()
                    else: sys.stdout.flush()
                    first = False
                    continue

            # å®Ÿå‡¦ç†
            if pdf_url:
                print(f"  PDF Found: {pdf_url}", file=sys.stderr)
                pdf_bytes = download_pdf(pdf_url)
                
                if pdf_bytes:
                    if args.pdf_only:
                        print("  Skipping Gemini extraction (--pdf-only)", file=sys.stderr)
                        extracted = None
                    else:
                        extracted = extract_prices_with_gemini(pdf_bytes)
                    if extracted and (extracted.get("weekday_30min_member") or extracted.get("weekday_30min_general")):
                        pricing_data = {
                            "status": "success",
                            "day": {
                                "30min": {
                                    "member": extracted.get("weekday_30min_member"), 
                                    "general": extracted.get("weekday_30min_general")
                                },
                                "free_time": {
                                    "member": extracted.get("weekday_free_time_member"), 
                                    "general": extracted.get("weekday_free_time_general")
                                }
                            }
                        }
                        print(f"  âœ… Extracted: 30min(Men/Gen)={extracted.get('weekday_30min_member')}/{extracted.get('weekday_30min_general')}", file=sys.stderr)
                        success_count += 1
                        if extracted.get("weekday_30min_general"):
                            general_price_count += 1
                    else:
                        print("  âŒ Extraction failed or returned null.", file=sys.stderr)
            else:
                print("  âŒ PDF not found.", file=sys.stderr)

            result = {
                "store_name": store['name'],
                "pdf_url": pdf_url,
                "pricing": pricing_data
            }
            
            if not first: print(",", file=out_stream)
            print(json.dumps(result, ensure_ascii=False, indent=2), file=out_stream)
            if args.output: out_stream.flush()
            else: sys.stdout.flush()
            first = False
            time.sleep(3) # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

        print("]", file=out_stream)
        
    finally:
        if args.output and out_stream != sys.stdout:
            out_stream.close()

    print(f"\nExample Stats: Success={success_count}, WithGeneralPrice={general_price_count}", file=sys.stderr)

if __name__ == "__main__":
    main()
